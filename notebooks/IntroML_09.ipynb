{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clasificación utilizando Naive Bayes\n",
        "\n",
        "Este ejemplo está basado en el Libro Python Data Science Handbook de Jake VanderPlas"
      ],
      "metadata": {
        "id": "gSucJxPZTiRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los modelos Naive Bayes (Bayes Ingenuo) son un grupo de algoritmos de clasificación extremadamente rápidos y sencillos que suelen ser adecuados para conjuntos de datos de muy alta dimensión.\n",
        "\n",
        "Al ser tan rápidos y tener tan pocos parámetros ajustables, acaban siendo útiles como referencia rápida y sucia para un problema de clasificación.\n",
        "\n",
        "En este cuaderno se ofrece una explicación intuitiva del funcionamiento de los clasificadores Bayes ingenuos, seguida de algunos ejemplos de su funcionamiento en algunos conjuntos de datos."
      ],
      "metadata": {
        "id": "wOr_yMNITn9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificación Bayesiana\n",
        "\n",
        "Los clasificadores Naive Bayes se basan en métodos de clasificación bayesianos.\n",
        "Se basan en el teorema de Bayes, que es una ecuación que describe la relación de probabilidades condicionales de cantidades estadísticas.\n",
        "En la clasificación bayesiana, nos interesa encontrar la probabilidad de una etiqueta $L$ dadas algunas características observadas, que podemos escribir como $P(L~|~{\\rm features})$.\n",
        "\n",
        "El teorema de Bayes nos dice cómo expresar esto en términos de cantidades que podemos calcular más directamente:\n",
        "\n",
        "$$\n",
        "P(L~|~{\\rm features}) = \\frac{P({\\rm features}~|~L)P(L)}{P({\\rm features})}\n",
        "$$\n",
        "\n",
        "Si intentamos decidir entre dos etiquetas -llamémoslas $L_1$ and $L_2$-entonces una forma de tomar esta decisión es calcular el cociente de las probabilidades posteriores para cada etiqueta:\n",
        "\n",
        "$$\n",
        "\\frac{P(L_1~|~{\\rm features})}{P(L_2~|~{\\rm features})} = \\frac{P({\\rm features}~|~L_1)}{P({\\rm features}~|~L_2)}\\frac{P(L_1)}{P(L_2)}\n",
        "$$\n",
        "\n",
        "Todo lo que necesitamos ahora es algún modelo por el cual podamos calcular $P({\\rm features}~|~L_i)$ para cada etiqueta.\n",
        "\n",
        "Este modelo se denomina *modelo generativo* porque especifica el proceso aleatorio hipotético que genera los datos.\n",
        "Especificar este modelo generativo para cada etiqueta es la pieza principal del entrenamiento de un clasificador bayesiano de este tipo.\n",
        "La versión general de dicho paso de entrenamiento es una tarea muy difícil, pero podemos simplificarla mediante el uso de algunas suposiciones simplificadoras sobre la forma de este modelo.\n",
        "\n",
        "Aquí es donde entra el \"ingenuo\" de \"Bayes ingenuo\": si hacemos suposiciones muy ingenuas sobre el modelo generativo para cada etiqueta, podemos encontrar una aproximación aproximada del modelo generativo para cada clase, y luego proceder con la clasificación bayesiana.\n",
        "Los distintos tipos de clasificadores Bayes ingenuos se basan en distintas suposiciones ingenuas sobre los datos, y examinaremos algunas de ellas en las secciones siguientes.\n",
        "\n",
        "Empezaremos con las importaciones estándar:"
      ],
      "metadata": {
        "id": "oXSKiZJsUR2E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqSVt0BBTSMc"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-whitegrid')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bayes ingenuo gaussiano\n",
        "\n",
        "Quizás el clasificador Bayes ingenuo más fácil de entender es el Bayes ingenuo gaussiano.\n",
        "Con este clasificador, la suposición es que *los datos de cada etiqueta se extraen de una distribución gaussiana simple*.\n",
        "Imaginemos que tenemos los siguientes datos:"
      ],
      "metadata": {
        "id": "UxO7rIQsVhYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu');"
      ],
      "metadata": {
        "id": "NbEm22sfV4iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo gaussiano más sencillo consiste en suponer que los datos se describen mediante una distribución gaussiana sin covarianza entre dimensiones. Este modelo puede ajustarse calculando la media y la desviación típica de los puntos dentro de cada etiqueta, que es todo lo que necesitamos para definir dicha distribución. El resultado de esta suposición gaussiana ingenua se muestra en la siguiente figura:"
      ],
      "metadata": {
        "id": "dGgSK0PYWDsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
        "ax.set_title('Naive Bayes Model', size=14)\n",
        "\n",
        "xlim = (-8, 8)\n",
        "ylim = (-15, 5)\n",
        "\n",
        "xg = np.linspace(xlim[0], xlim[1], 60)\n",
        "yg = np.linspace(ylim[0], ylim[1], 40)\n",
        "xx, yy = np.meshgrid(xg, yg)\n",
        "Xgrid = np.vstack([xx.ravel(), yy.ravel()]).T\n",
        "\n",
        "for label, color in enumerate(['red', 'blue']):\n",
        "    mask = (y == label)\n",
        "    mu, std = X[mask].mean(0), X[mask].std(0)\n",
        "    P = np.exp(-0.5 * (Xgrid - mu) ** 2 / std ** 2).prod(1)\n",
        "    Pm = np.ma.masked_array(P, P < 0.03)\n",
        "    ax.pcolorfast(xg, yg, Pm.reshape(xx.shape), alpha=0.5,\n",
        "                  cmap=color.title() + 's')\n",
        "    ax.contour(xx, yy, P.reshape(xx.shape),\n",
        "               levels=[0.01, 0.1, 0.5, 0.9],\n",
        "               colors=color, alpha=0.2)\n",
        "\n",
        "ax.set(xlim=xlim, ylim=ylim)"
      ],
      "metadata": {
        "id": "feTLvdztWvpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las elipses representan el modelo generativo gaussiano para cada etiqueta, con una probabilidad mayor hacia el centro de las elipses. Con este modelo generativo para cada clase, tenemos una receta sencilla para calcular la probabilidad $P({\\rm features}~|~L_1)$ para cualquier punto de datos, y así podemos calcular rápidamente la proporción posterior y determinar qué etiqueta es la más probable para un punto dado.\n",
        "\n",
        "Este procedimiento se implementa en el estimador `sklearn.naive_bayes.GaussianNB` de Scikit-Learn:"
      ],
      "metadata": {
        "id": "EXSR_Lj9W8jU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "model = GaussianNB()\n",
        "model.fit(X, y);"
      ],
      "metadata": {
        "id": "SQQ17m11XAvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generemos nuevos datos y predigamos la etiqueta:"
      ],
      "metadata": {
        "id": "n6euf2opXPac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.RandomState(0)\n",
        "Xnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\n",
        "ynew = model.predict(Xnew)"
      ],
      "metadata": {
        "id": "q89fRRHnXTjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora podemos trazar estos nuevos datos para hacernos una idea de dónde está el límite de decisión (véase la siguiente figura):"
      ],
      "metadata": {
        "id": "VUBi_zo6XZ6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
        "lim = plt.axis()\n",
        "plt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=20, cmap='RdBu', alpha=0.1)\n",
        "plt.axis(lim);"
      ],
      "metadata": {
        "id": "DwI0Z5-mXdUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos un límite ligeramente curvo en las clasificaciones; en general, el límite producido por un modelo Bayesiano ingenuo será cuadrático.\n",
        "\n",
        "Un aspecto interesante de este formalismo bayesiano es que permite de forma natural la clasificación probabilística, que podemos calcular utilizando el método `predict_proba`:"
      ],
      "metadata": {
        "id": "l1aPIeoOXwMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yprob = model.predict_proba(Xnew)\n",
        "yprob[-8:].round(2)"
      ],
      "metadata": {
        "id": "oW__uRv7XzYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las columnas dan las probabilidades posteriores de la primera y segunda etiquetas, respectivamente.\n",
        "Si busca estimaciones de la incertidumbre en su clasificación, los enfoques bayesianos como éste pueden ser un buen punto de partida.\n",
        "\n",
        "Por supuesto, la clasificación final sólo será tan buena como las suposiciones del modelo que conducen a ella, por lo que el Bayes ingenuo gaussiano a menudo no produce muy buenos resultados.\n",
        "Aun así, en muchos casos -especialmente cuando el número de características es elevado- esta suposición no es lo suficientemente perjudicial como para impedir que Gaussian naive Bayes sea un método fiable."
      ],
      "metadata": {
        "id": "bxkzBRq-X9pb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multinomial Naive Bayes\n",
        "\n",
        "La hipótesis gaussiana que acabamos de describir no es, ni mucho menos, la única hipótesis sencilla que podría utilizarse para especificar la distribución generativa de cada etiqueta.\n",
        "Otro ejemplo útil es el Bayes ingenuo multinomial, en el que se supone que las características se generan a partir de una distribución multinomial simple.\n",
        "La distribución multinomial describe la probabilidad de observar recuentos entre una serie de categorías, por lo que el Bayes ingenuo multinomial es el más adecuado para las características que representan recuentos o tasas de recuento.\n",
        "\n",
        "La idea es exactamente la misma que antes, salvo que en lugar de modelar la distribución de datos con la gaussiana de mejor ajuste, la modelamos con una distribución multinomial de mejor ajuste."
      ],
      "metadata": {
        "id": "TrVc0C5qX_Mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo: Clasificación de texto\n",
        "\n",
        "Uno de los ámbitos en los que se suele utilizar el método Bayes ingenuo multinomial es en la clasificación de textos, donde las características están relacionadas con el recuento de palabras o las frecuencias dentro de los documentos que se van a clasificar.\n",
        "\n",
        "Aquí utilizaremos las características de recuento de palabras dispersas del corpus 20 Newsgroups, disponible a través de Scikit-Learn, para mostrar cómo podemos clasificar estos documentos breves en categorías.\n",
        "\n",
        "Descarguemos los datos y echemos un vistazo a los nombres objetivo:"
      ],
      "metadata": {
        "id": "DKdtqGQ6YWBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "data = fetch_20newsgroups()\n",
        "data.target_names"
      ],
      "metadata": {
        "id": "AcYh9RA-Ydkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para simplificar, seleccionaremos sólo algunas de estas categorías y descargaremos los conjuntos de entrenamiento y prueba:"
      ],
      "metadata": {
        "id": "VSJu13_FYsVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories = ['talk.religion.misc', 'soc.religion.christian',\n",
        "              'sci.space', 'comp.graphics']\n",
        "train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "test = fetch_20newsgroups(subset='test', categories=categories)"
      ],
      "metadata": {
        "id": "PQuwdyyYYwBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "He aquí una entrada representativa de los datos:"
      ],
      "metadata": {
        "id": "-Qo0Za_HY0Eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.data[7][25:])"
      ],
      "metadata": {
        "id": "bvbD8CJKY1Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con el fin de utilizar estos datos para el aprendizaje automático, tenemos que ser capaces de convertir el contenido de cada cadena en un vector de números.\n",
        "Para ello, utilizaremos el vectorizador TF-IDF, y crearemos un canal (pipeline) que lo conecte a un clasificador Bayes ingenuo multinomial:"
      ],
      "metadata": {
        "id": "qgar_rUuZODs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB())"
      ],
      "metadata": {
        "id": "aYUz6OS2ZU7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con este proceso, podemos aplicar el modelo a los datos de entrenamiento y predecir las etiquetas de los datos de prueba:"
      ],
      "metadata": {
        "id": "g4bAxwkQZcIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train.data, train.target)\n",
        "labels = model.predict(test.data)"
      ],
      "metadata": {
        "id": "JX_N6YzCZe9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora que hemos predicho las etiquetas de los datos de prueba, podemos evaluarlas para conocer el rendimiento del estimador.\n",
        "Por ejemplo, echemos un vistazo a la matriz de confusión entre las etiquetas verdaderas y las predichas para los datos de prueba (véase la figura siguiente):"
      ],
      "metadata": {
        "id": "9BzDfz7dZjfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "mat = confusion_matrix(test.target, labels)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
        "            xticklabels=train.target_names, yticklabels=train.target_names,\n",
        "            cmap='Blues')\n",
        "plt.xlabel('Etiqueta Verdadera')\n",
        "plt.ylabel('Etiqueta Predicha');"
      ],
      "metadata": {
        "id": "Dy5g3t-QZkaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evidentemente, incluso este clasificador tan simple puede separar con éxito las discusiones espaciales de las informáticas, pero se confunde entre las discusiones sobre religión y las discusiones sobre cristianismo.\n",
        "Quizás era de esperar.\n",
        "\n",
        "Lo bueno aquí es que ahora tenemos las herramientas para determinar la categoría de *cualquier* cadena, usando el método `predict` de este pipeline.\n",
        "He aquí una función de utilidad que devolverá la predicción para una sola cadena:"
      ],
      "metadata": {
        "id": "LIB1eu6BZ6-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_category(s, train=train, model=model):\n",
        "    pred = model.predict([s])\n",
        "    return train.target_names[pred[0]]"
      ],
      "metadata": {
        "id": "7HXc3HUBaAZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pongámosla a prueba:"
      ],
      "metadata": {
        "id": "qPpBWTdraCxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_category('sending an payload to the ISS')"
      ],
      "metadata": {
        "id": "ZNhkGGjnaFnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_category('God is everywhere')"
      ],
      "metadata": {
        "id": "s2olyBdEaSJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_category('I need to restar my computer')"
      ],
      "metadata": {
        "id": "TfuARVTraYE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recuerde que no se trata de nada más sofisticado que un simple modelo de probabilidad para la frecuencia (ponderada) de cada palabra de la cadena; no obstante, el resultado es sorprendente.\n",
        "\n",
        "Incluso un algoritmo muy ingenuo, cuando se utiliza con cuidado y se entrena con un gran conjunto de datos de alta dimensión, puede ser sorprendentemente eficaz."
      ],
      "metadata": {
        "id": "E1q5-a9-afOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cuándo utilizar Naive Bayes\n",
        "\n",
        "Dado que los clasificadores Bayes ingenuos hacen suposiciones muy estrictas sobre los datos, no suelen funcionar tan bien como los modelos más complicados.\n",
        "Dicho esto, tienen varias ventajas:\n",
        "\n",
        "- Son rápidos tanto en el entrenamiento como en la predicción.\n",
        "- Proporcionan una predicción probabilística directa.\n",
        "- Suelen ser fáciles de interpretar.\n",
        "- Tienen pocos (o ningún) parámetro ajustable.\n",
        "\n",
        "Estas ventajas significan que un clasificador Bayes ingenuo es a menudo una buena opción como clasificación inicial de referencia.\n",
        "Si funciona correctamente, enhorabuena: tiene un clasificador muy rápido y muy interpretable para su problema.\n",
        "Si no funciona bien, puede empezar a explorar modelos más sofisticados, con un cierto conocimiento de referencia sobre su rendimiento.\n",
        "\n",
        "Los clasificadores Bayes ingenuos tienden a funcionar especialmente bien en las siguientes situaciones:\n",
        "\n",
        "- Cuando las suposiciones ingenuas coinciden realmente con los datos (muy raro en la práctica).\n",
        "- Para categorías muy bien separadas, cuando la complejidad del modelo es menos importante.\n",
        "- Para datos de muy alta dimensión, cuando la complejidad del modelo es menos importante.\n",
        "\n",
        "Los dos últimos puntos parecen distintos, pero en realidad están relacionados: a medida que aumenta la dimensionalidad de un conjunto de datos, es mucho menos probable que dos puntos cualesquiera se encuentren próximos entre sí (después de todo, deben estar próximos en *cada una de las dimensiones* para estar próximos en conjunto).\n",
        "Esto significa que los conglomerados en dimensiones altas tienden a estar más separados, por término medio, que los conglomerados en dimensiones bajas, suponiendo que las nuevas dimensiones realmente añadan información.\n",
        "Por este motivo, los clasificadores simplistas, como los que se comentan aquí, tienden a funcionar tan bien o mejor que los clasificadores más complicados a medida que aumenta la dimensionalidad: una vez que se tienen suficientes datos, incluso un modelo simple puede ser muy potente."
      ],
      "metadata": {
        "id": "-0Qvmzwxamx8"
      }
    }
  ]
}